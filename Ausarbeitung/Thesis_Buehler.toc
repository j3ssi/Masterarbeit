\babel@toc {german}{}
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Motivation und Hintergrund dieser Arbeit}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Aufbau der Arbeit}{1}{section.1.2}
\contentsline {section}{\numberline {1.3}Suchbegriffe}{1}{section.1.3}
\contentsline {chapter}{\numberline {2}Stand der Wissenschaft}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Funktionsweise eines CNNs}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}Strukturelle Methoden}{5}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}MorphNet}{6}{subsection.2.2.1}
\contentsline {section}{\numberline {2.3}Zeitsparende Nethoden}{6}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Verringerung der f\IeC {\"u}r Berechnungen n\IeC {\"o}tige Zeit}{6}{subsection.2.3.1}
\contentsline {paragraph}{\nonumberline Berechnung mit 16 Bit Gleitkomma}{6}{section*.10}
\contentsline {subsection}{\numberline {2.3.2}Beschleunigung der Berechnung des Gradientenabstiegsverfahren}{8}{subsection.2.3.2}
\contentsline {subsubsection}{\nonumberline Accelerating CNN Training by Sparsifying Activation Gradients}{8}{section*.15}
\contentsline {subsubsection}{\nonumberline Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{8}{section*.16}
\contentsline {subsubsection}{\nonumberline Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{8}{section*.17}
\contentsline {subsubsection}{\nonumberline Accelerated CNN Training Through Gradient Approximation}{8}{section*.18}
\contentsline {section}{\numberline {2.4}Verfahren um weniger Trainingsdaten zu verwenden}{8}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Stochastisches Pooling}{8}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Lernen von Struktur und St\IeC {\"a}rke von CNNs}{8}{subsection.2.4.2}
\contentsline {section}{\numberline {2.5}Strukturelle Ver\IeC {\"a}nderungen zur Beschleunigung des Trainings}{8}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Pruning um Trainingszeit zu minimieren}{8}{subsection.2.5.1}
\contentsline {subsubsection}{\nonumberline Prune Train}{9}{section*.19}
\contentsline {subsubsection}{\nonumberline The Lottery Ticket Hypothesis}{11}{section*.20}
\contentsline {subsection}{\numberline {2.5.2}Net 2 Net}{11}{subsection.2.5.2}
\contentsline {subsection}{\numberline {2.5.3}Kernel rescaling}{11}{subsection.2.5.3}
\contentsline {subsection}{\numberline {2.5.4}Resource Aware Layer Replacement}{11}{subsection.2.5.4}
\contentsline {section}{\numberline {2.6}Weitere Herangehensweisen}{11}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Tree CNN}{11}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Standardization Loss}{11}{subsection.2.6.2}
\contentsline {subsection}{\numberline {2.6.3}Wavelet}{11}{subsection.2.6.3}
\contentsline {chapter}{\numberline {3}Experimente -- Arbeitstitel}{13}{chapter.3}
\contentsline {section}{\numberline {3.1}Experimentales Setup}{13}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Wahl des Frameworks}{13}{subsection.3.1.1}
\contentsline {section}{\numberline {3.2}Untersuchung von PruneTrain}{13}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Aufbau des CNNs}{13}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Einfluss der Batch Gr\IeC {\"o}\IeC {\ss }e}{14}{subsection.3.2.2}
\contentsline {subsection}{\numberline {3.2.3}Einfluss der Batchgr\IeC {\"o}sse und der Lernrate auf die Verkleinerung des Netzes}{15}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Nachvollziehbarkeit der PruneTrain Ergebnisse}{16}{subsection.3.2.4}
\contentsline {section}{\numberline {3.3}PruneTrain als MorphNet}{16}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Net 2 Net}{16}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Morphnet}{16}{subsection.3.3.2}
\contentsline {section}{\numberline {3.4}\IeC {\"U}berblick \IeC {\"u}ber die m\IeC {\"o}glichen Strategien}{16}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Zahlenformate}{17}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Beschleunigung der Berechnung des Gradientenabstiegverfahren}{17}{subsection.3.4.2}
\contentsline {subsubsection}{\nonumberline Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{18}{section*.35}
\contentsline {subsubsection}{\nonumberline Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{18}{section*.37}
\contentsline {subsubsection}{\nonumberline Accelerated CNN Training Through Gradient Approximation}{18}{section*.39}
\contentsline {subsubsection}{\nonumberline Sind diese Verfahren theoretisch kombinierbar}{19}{section*.40}
\contentsline {subsection}{\numberline {3.4.3}Verfahren um weniger Trainingsdaten zu verwenden}{19}{subsection.3.4.3}
\contentsline {subsubsection}{\nonumberline Stochastisches Pooling}{19}{section*.42}
\contentsline {subsection}{\numberline {3.4.4}Lernen von Struktur und St\IeC {\"a}rke von CNNs}{19}{subsection.3.4.4}
\contentsline {section}{\numberline {3.5}Schnelleres MorphPruneTrain}{19}{section.3.5}
\contentsline {chapter}{\numberline {4}Evaluation}{21}{chapter.4}
\contentsline {part}{\numberline {I}Additional information}{23}{part.1}
\contentsline {chapter}{Abbildungsverzeichnis}{25}{chapter*.46}
\contentsline {chapter}{Algorithmenverzeichnis}{27}{chapter*.47}
\contentsline {chapter}{Quellcodeverzeichnis}{29}{chapter*.48}
\contentsline {chapter}{Literaturverzeichnis}{31}{chapter*.48}
