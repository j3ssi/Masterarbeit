\babel@toc {german}{}
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Motivation und Hintergrund dieser Arbeit}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Aufbau der Arbeit}{1}{section.1.2}
\contentsline {chapter}{\numberline {2}Stand der Wissenschaft}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Funktionsweise eines CNN}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}ResNet -- eine neuere CNN-Architektur}{7}{section.2.2}
\contentsline {section}{\numberline {2.3}Vorgehen zur Suche nachdem Stand der Wissenschaft}{9}{section.2.3}
\contentsline {section}{\numberline {2.4}Beschneidung des Netzes zur Beschleunigung des Training}{11}{section.2.4}
\contentsline {section}{\numberline {2.5}Beschleunigung des Lernens durch Wissenstransfer}{14}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Operator f\IeC {\"u}r breiteres Netz}{15}{subsection.2.5.1}
\contentsline {subsubsection}{\nonumberline Tieferes Netz}{16}{section*.15}
\contentsline {subsubsection}{\nonumberline Diskussion der Methode}{17}{section*.16}
\contentsline {section}{\numberline {2.6}Schnelles Ressourcen beschr\IeC {\"a}nktes Strukturlernen tiefer Netzwerke}{17}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Definition der Nebenbedingung}{18}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Regularsierer}{19}{subsection.2.6.2}
\contentsline {section}{\numberline {2.7}Automatische Architektursuche}{20}{section.2.7}
\contentsline {section}{\numberline {2.8}Zeitsparende Nethoden}{21}{section.2.8}
\contentsline {subsection}{\numberline {2.8.1}Verringerung der f\IeC {\"u}r Berechnungen n\IeC {\"o}tige Zeit}{21}{subsection.2.8.1}
\contentsline {paragraph}{\nonumberline Berechnung mit 16 Bit Gleitkomma}{21}{section*.19}
\contentsline {subsection}{\numberline {2.8.2}Beschleunigung der Berechnung des Gradientenabstiegsverfahren}{23}{subsection.2.8.2}
\contentsline {subsubsection}{\nonumberline Accelerating CNN Training by Sparsifying Activation Gradients}{23}{section*.25}
\contentsline {subsubsection}{\nonumberline Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{23}{section*.26}
\contentsline {subsubsection}{\nonumberline Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{23}{section*.27}
\contentsline {subsubsection}{\nonumberline Accelerated CNN Training Through Gradient Approximation}{23}{section*.28}
\contentsline {section}{\numberline {2.9}Additive Methoden}{23}{section.2.9}
\contentsline {subsection}{\numberline {2.9.1}Verfahren zum Verwenden maximaler Batchgr\IeC {\"o}\IeC {\ss }en}{23}{subsection.2.9.1}
\contentsline {chapter}{\numberline {3}Experimente -- Arbeitstitel}{25}{chapter.3}
\contentsline {section}{\numberline {3.1}Experimentales Setup}{25}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Hardware}{25}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Wahl des Frameworks}{25}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}verwendete Netzarchitektur}{26}{subsection.3.1.3}
\contentsline {subsection}{\numberline {3.1.4}\IeC {\"U}berblick \IeC {\"u}ber das experimentelle Vorgehen}{26}{subsection.3.1.4}
\contentsline {section}{\numberline {3.2}Untersuchung von PruneTrain}{26}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Evaluation der vorgefertigten Implementierung}{27}{subsection.3.2.1}
\contentsline {subsubsection}{\nonumberline Experimente zur Lernrate}{27}{section*.35}
\contentsline {subsubsection}{\nonumberline Experimente zum Rekonfigurationsintervall}{28}{section*.38}
\contentsline {subsubsection}{\nonumberline Experimente zum Grenzwert}{29}{section*.41}
\contentsline {subsubsection}{\nonumberline Experimente zur Batchgr\IeC {\"o}\IeC {\ss }e}{30}{section*.44}
\contentsline {subsubsection}{\nonumberline Ver\IeC {\"a}nderung der Accuracy durch PruneTrain}{31}{section*.47}
\contentsline {subsection}{\numberline {3.2.2}Einfluss der Batchgr\IeC {\"o}\IeC {\ss }e auf PruneTrain}{32}{subsection.3.2.2}
\contentsline {subsubsection}{\nonumberline LARS}{37}{section*.60}
\contentsline {section}{\numberline {3.3}Untersuchung von Net2Net}{37}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Evaluierung des Operators f\IeC {\"u}r ein breiteres Netz}{37}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Evaluierung des Operators f\IeC {\"u}r ein tieferes Netz}{38}{subsection.3.3.2}
\contentsline {section}{\numberline {3.4}Untersuchung von MorphNet}{38}{section.3.4}
\contentsline {section}{\numberline {3.5}PruneTrain + Net2Net}{38}{section.3.5}
\contentsline {section}{\numberline {3.6}Additive Verfahren}{38}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Zahlenformate}{38}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Beschleunigung der Berechnung des Gradientenabstiegverfahren}{39}{subsection.3.6.2}
\contentsline {subsubsection}{\nonumberline Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{39}{section*.68}
\contentsline {subsubsection}{\nonumberline Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{40}{section*.69}
\contentsline {subsubsection}{\nonumberline Accelerated CNN Training Through Gradient Approximation}{40}{section*.70}
\contentsline {chapter}{\numberline {4}Evaluation}{41}{chapter.4}
\contentsline {chapter}{\numberline {5}Ausblick und Fazit}{43}{chapter.5}
\contentsline {chapter}{\numberline {A}d}{45}{appendix.A}
\contentsline {chapter}{Abbildungsverzeichnis}{47}{chapter*.72}
\contentsline {chapter}{Literaturverzeichnis}{49}{chapter*.72}
