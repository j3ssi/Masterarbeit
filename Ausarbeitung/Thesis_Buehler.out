\BOOKMARK [0][]{pdf:title.0}{Front page}{}% 1
\BOOKMARK [0][]{pdf:toc.0}{Inhaltsverzeichnis}{}% 2
\BOOKMARK [0][]{pdf:Notation.0}{Mathematische Notation}{}% 3
\BOOKMARK [0][]{chapter.1}{Einleitung}{}% 4
\BOOKMARK [1][-]{section.1.1}{Motivation und Hintergrund dieser Arbeit}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.2}{Aufbau der Arbeit}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.3}{Suchbegriffe}{chapter.1}% 7
\BOOKMARK [0][]{chapter.2}{Stand der Wissenschaft}{}% 8
\BOOKMARK [1][-]{section.2.1}{Funktionsweise eines CNNs}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.2}{\334berblick \374ber die g\344ngigen Methoden}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.2.1}{Verringerung der f\374r Berechnungen n\366tige Zeit}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.2}{Berechnung mit 16 Bit Dynamischen Festkommazahlen}{section.2.2}% 12
\BOOKMARK [1][-]{section.2.3}{Beschleunigung der Berechnung des Gradientenabstiegsverfahren}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.3.1}{Accelerating CNN Training by Sparsifying Activation Gradients}{section.2.3}% 14
\BOOKMARK [2][-]{subsection.2.3.2}{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{section.2.3}% 15
\BOOKMARK [2][-]{subsection.2.3.3}{Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{section.2.3}% 16
\BOOKMARK [2][-]{subsection.2.3.4}{Accelerated CNN Training Through Gradient Approximation}{section.2.3}% 17
\BOOKMARK [1][-]{section.2.4}{Verfahren um weniger Trainingsdaten zu verwenden}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.4.1}{Stochastisches Pooling}{section.2.4}% 19
\BOOKMARK [2][-]{subsection.2.4.2}{Lernen von Struktur und St\344rke von CNNs}{section.2.4}% 20
\BOOKMARK [1][-]{section.2.5}{Strukturelle Ver\344nderungen zur Beschleunigung des Trainings}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.5.1}{Pruning um Trainingszeit zu minimieren}{section.2.5}% 22
\BOOKMARK [2][-]{subsection.2.5.2}{Net 2 Net}{section.2.5}% 23
\BOOKMARK [2][-]{subsection.2.5.3}{Kernel rescaling}{section.2.5}% 24
\BOOKMARK [2][-]{subsection.2.5.4}{Resource Aware Layer Replacement}{section.2.5}% 25
\BOOKMARK [1][-]{section.2.6}{Weitere Herangehensweisen}{chapter.2}% 26
\BOOKMARK [2][-]{subsection.2.6.1}{Tree CNN}{section.2.6}% 27
\BOOKMARK [2][-]{subsection.2.6.2}{Standardization Loss}{section.2.6}% 28
\BOOKMARK [2][-]{subsection.2.6.3}{Wavelet}{section.2.6}% 29
\BOOKMARK [0][]{chapter.3}{Experimentelle Untersuchung der m\366glichen Strategien}{}% 30
\BOOKMARK [1][-]{section.3.1}{Experimentales Setup}{chapter.3}% 31
\BOOKMARK [1][-]{section.3.2}{\334berblick \374ber die m\366glichen Strategien}{chapter.3}% 32
\BOOKMARK [2][-]{subsection.3.2.1}{Zahlenformate}{section.3.2}% 33
\BOOKMARK [2][-]{subsection.3.2.2}{Beschleunigung der Berechnung des Gradientenabstiegverfahren}{section.3.2}% 34
\BOOKMARK [2][-]{subsection.3.2.3}{Verfahren um weniger Trainingsdaten zu verwenden}{section.3.2}% 35
\BOOKMARK [2][-]{subsection.3.2.4}{Lernen von Struktur und St\344rke von CNNs}{section.3.2}% 36
\BOOKMARK [2][-]{subsection.3.2.5}{Strukturelle Ver\344nderungen}{section.3.2}% 37
\BOOKMARK [2][-]{subsection.3.2.6}{andere Herangehensweisen}{section.3.2}% 38
\BOOKMARK [2][-]{subsection.3.2.7}{Tensorflow vs. PyTorch}{section.3.2}% 39
\BOOKMARK [1][-]{section.3.3}{Durchf\374hrung der Experimente}{chapter.3}% 40
\BOOKMARK [2][-]{subsection.3.3.1}{Einfluss der Batch Gr\366\337e}{section.3.3}% 41
\BOOKMARK [2][-]{subsection.3.3.2}{Kombination von Net2Net mit PruneTrain}{section.3.3}% 42
\BOOKMARK [1][-]{section.3.4}{Evaluation der Ergebenisse}{chapter.3}% 43
\BOOKMARK [0][]{chapter.4}{Konklusion}{}% 44
\BOOKMARK [-1][]{part.1}{Additional information}{}% 45
\BOOKMARK [0][]{chapter*.25}{Abbildungsverzeichnis}{part.1}% 46
\BOOKMARK [0][]{chapter*.26}{Algorithmenverzeichnis}{part.1}% 47
\BOOKMARK [0][]{chapter*.27}{Quellcodeverzeichnis}{part.1}% 48
\BOOKMARK [0][]{chapter*.27}{Literaturverzeichnis}{part.1}% 49
