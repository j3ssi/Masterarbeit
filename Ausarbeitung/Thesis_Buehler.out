\BOOKMARK [0][]{chapter.1}{Einleitung}{}% 1
\BOOKMARK [1][-]{section.1.1}{Motivation\040und\040Hintergrund\040dieser\040Arbeit}{chapter.1}% 2
\BOOKMARK [1][-]{section.1.2}{Aufbau\040der\040Arbeit}{chapter.1}% 3
\BOOKMARK [1][-]{section.1.3}{Suchbegriffe}{chapter.1}% 4
\BOOKMARK [0][]{chapter.2}{Stand\040der\040Wissenschaft}{}% 5
\BOOKMARK [1][-]{section.2.1}{Funktionsweise\040eines\040CNNs}{chapter.2}% 6
\BOOKMARK [1][-]{section.2.2}{ResNet\040\205\040eine\040neuere\040CNN-Architektur}{chapter.2}% 7
\BOOKMARK [1][-]{section.2.3}{Strukturelle\040Methoden}{chapter.2}% 8
\BOOKMARK [2][-]{subsection.2.3.1}{MorphNet}{section.2.3}% 9
\BOOKMARK [1][-]{section.2.4}{Zeitsparende\040Nethoden}{chapter.2}% 10
\BOOKMARK [2][-]{subsection.2.4.1}{Verringerung\040der\040f\374r\040Berechnungen\040n\366tige\040Zeit}{section.2.4}% 11
\BOOKMARK [2][-]{subsection.2.4.2}{Beschleunigung\040der\040Berechnung\040des\040Gradientenabstiegsverfahren}{section.2.4}% 12
\BOOKMARK [1][-]{section.2.5}{Verfahren\040um\040weniger\040Trainingsdaten\040zu\040verwenden}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.5.1}{Stochastisches\040Pooling}{section.2.5}% 14
\BOOKMARK [2][-]{subsection.2.5.2}{Lernen\040von\040Struktur\040und\040St\344rke\040von\040CNNs}{section.2.5}% 15
\BOOKMARK [1][-]{section.2.6}{Strukturelle\040Ver\344nderungen\040zur\040Beschleunigung\040des\040Trainings}{chapter.2}% 16
\BOOKMARK [2][-]{subsection.2.6.1}{Pruning\040um\040Trainingszeit\040zu\040minimieren}{section.2.6}% 17
\BOOKMARK [2][-]{subsection.2.6.2}{Net\0402\040Net}{section.2.6}% 18
\BOOKMARK [2][-]{subsection.2.6.3}{Kernel\040rescaling}{section.2.6}% 19
\BOOKMARK [2][-]{subsection.2.6.4}{Resource\040Aware\040Layer\040Replacement}{section.2.6}% 20
\BOOKMARK [1][-]{section.2.7}{Weitere\040Herangehensweisen}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.7.1}{Tree\040CNN}{section.2.7}% 22
\BOOKMARK [2][-]{subsection.2.7.2}{Standardization\040Loss}{section.2.7}% 23
\BOOKMARK [2][-]{subsection.2.7.3}{Wavelet}{section.2.7}% 24
\BOOKMARK [0][]{chapter.3}{Experimente\040\205\040Arbeitstitel}{}% 25
\BOOKMARK [1][-]{section.3.1}{Experimentales\040Setup}{chapter.3}% 26
\BOOKMARK [2][-]{subsection.3.1.1}{Hardware}{section.3.1}% 27
\BOOKMARK [2][-]{subsection.3.1.2}{Wahl\040des\040Frameworks}{section.3.1}% 28
\BOOKMARK [1][-]{section.3.2}{Untersuchung\040von\040PruneTrain}{chapter.3}% 29
\BOOKMARK [2][-]{subsection.3.2.1}{verwendete\040Netzarchitektur}{section.3.2}% 30
\BOOKMARK [2][-]{subsection.3.2.2}{Implementierung\040der\040Anpassung\040der\040Batch\040Gr\366\337e}{section.3.2}% 31
\BOOKMARK [2][-]{subsection.3.2.3}{Einfluss\040der\040Batchgr\366sse\040und\040der\040Lernrate\040auf\040die\040Verkleinerung\040des\040Netzes}{section.3.2}% 32
\BOOKMARK [2][-]{subsection.3.2.4}{Nachvollziehbarkeit\040der\040PruneTrain\040Ergebnisse}{section.3.2}% 33
\BOOKMARK [1][-]{section.3.3}{PruneTrain\040als\040MorphNet}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.3.1}{Net\0402\040Net}{section.3.3}% 35
\BOOKMARK [2][-]{subsection.3.3.2}{Morphnet}{section.3.3}% 36
\BOOKMARK [1][-]{section.3.4}{\334berblick\040\374ber\040die\040m\366glichen\040Strategien}{chapter.3}% 37
\BOOKMARK [2][-]{subsection.3.4.1}{Zahlenformate}{section.3.4}% 38
\BOOKMARK [2][-]{subsection.3.4.2}{Beschleunigung\040der\040Berechnung\040des\040Gradientenabstiegverfahren}{section.3.4}% 39
\BOOKMARK [2][-]{subsection.3.4.3}{Verfahren\040um\040weniger\040Trainingsdaten\040zu\040verwenden}{section.3.4}% 40
\BOOKMARK [2][-]{subsection.3.4.4}{Lernen\040von\040Struktur\040und\040St\344rke\040von\040CNNs}{section.3.4}% 41
\BOOKMARK [1][-]{section.3.5}{Schnelleres\040MorphPruneTrain}{chapter.3}% 42
\BOOKMARK [0][]{chapter.4}{Evaluation}{}% 43
\BOOKMARK [-1][]{part.1}{Additional\040information}{}% 44
\BOOKMARK [0][]{chapter*.49}{Abbildungsverzeichnis}{part.1}% 45
\BOOKMARK [0][]{chapter*.50}{Algorithmenverzeichnis}{part.1}% 46
\BOOKMARK [0][]{chapter*.51}{Quellcodeverzeichnis}{part.1}% 47
\BOOKMARK [0][]{chapter*.51}{Literaturverzeichnis}{part.1}% 48
