\babel@toc {german}{}
\contentsline {chapter}{\numberline {1}Einleitung}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Funktionsweise eines CNN}{1}{section.1.1}
\contentsline {chapter}{\numberline {2}Stand der Wissenschaft}{5}{chapter.2}
\contentsline {subsection}{\numberline {2.0.1}Suchbegriffe}{5}{subsection.2.0.1}
\contentsline {subsection}{\numberline {2.0.2}verwendete Datensets}{5}{subsection.2.0.2}
\contentsline {section}{\numberline {2.1}Verringerung der f\IeC {\"u}r Berechnungen n\IeC {\"o}tige Zeit}{6}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Berechnung mit 16 Bit Gleitkomma}{6}{subsection.2.1.1}
\contentsline {subsubsection}{\nonumberline 32 Bit Mastergewichte und Updates}{6}{section*.10}
\contentsline {subsubsection}{\nonumberline Sklaierung der Loss-Funktion}{7}{section*.13}
\contentsline {subsubsection}{\nonumberline Arithmetische Pr\IeC {\"a}zision}{7}{section*.14}
\contentsline {subsection}{\numberline {2.1.2}Berechnung mit 16 Bit Dynamischen Festkommazahlen}{7}{subsection.2.1.2}
\contentsline {chapter}{\numberline {3}Beschleunigung der Berechnung des Gradientenabstiegsverfahren}{9}{chapter.3}
\contentsline {section}{\numberline {3.1}Accelerating CNN Training by Sparsifying Activation Gradients}{9}{section.3.1}
\contentsline {section}{\numberline {3.2}Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{10}{section.3.2}
\contentsline {section}{\numberline {3.3}Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{10}{section.3.3}
\contentsline {section}{\numberline {3.4}Accelerated CNN Training Through Gradient Approximation}{10}{section.3.4}
\contentsline {chapter}{\numberline {4}Verfahren um weniger Trainingsdaten zu verwenden}{11}{chapter.4}
\contentsline {section}{\numberline {4.1}Stochastisches Pooling}{11}{section.4.1}
\contentsline {section}{\numberline {4.2}Lernen von Struktur und St\IeC {\"a}rke von CNNs}{11}{section.4.2}
\contentsline {chapter}{\numberline {5}Strukturelle Ver\IeC {\"a}nderungen zur Beschleunigung des Trainings}{13}{chapter.5}
\contentsline {section}{\numberline {5.1}Pruning um Trainingszeit zu minimieren}{13}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Prune Train}{13}{subsection.5.1.1}
\contentsline {section}{\numberline {5.2}Net 2 Net}{15}{section.5.2}
\contentsline {section}{\numberline {5.3}Kernel rescaling}{15}{section.5.3}
\contentsline {section}{\numberline {5.4}Resource Aware Layer Replacement}{15}{section.5.4}
\contentsline {chapter}{\numberline {6}Weitere Herangehensweisen}{17}{chapter.6}
\contentsline {section}{\numberline {6.1}Tree CNN}{17}{section.6.1}
\contentsline {section}{\numberline {6.2}Standardization Loss}{17}{section.6.2}
\contentsline {section}{\numberline {6.3}Wavelet}{17}{section.6.3}
\contentsline {part}{\numberline {I}Praktischer Teil-- Arbeitstitel}{19}{part.1}
\contentsline {chapter}{\numberline {7}Einleitung}{21}{chapter.7}
\contentsline {chapter}{\numberline {8}Durchf\IeC {\"u}hrung}{23}{chapter.8}
\contentsline {part}{\numberline {II}Additional information}{25}{part.2}
\contentsline {chapter}{Abbildungsverzeichnis}{27}{chapter*.15}
\contentsline {chapter}{Algorithmenverzeichnis}{29}{chapter*.16}
\contentsline {chapter}{Quellcodeverzeichnis}{31}{chapter*.17}
\contentsline {chapter}{Literaturverzeichnis}{33}{chapter*.17}
