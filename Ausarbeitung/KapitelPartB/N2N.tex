
\chapter{Evaluierung von Net2Net}\label{sec:net2netexperimente}
Die Operatoren zur Beschleunigung des Lernens durch Wissenstransfer werden in diesem Kapitel evaluiert. Diese Evaluierung arbeitet mit einer selbst erstellten Implementierung auf Grundlage der Veröffentlichung zum Thema Net2Net \cite{net2net}.

Die Evaluierung umfasst drei unterschiedliche Situationen, diese Situation sind ähnlich, wie in der dazugehörigen Quelle \cite{net2net}. Die Evaluierung arbeitet mit einem ResNet, wie in Kapitel \ref{sec:baseline}. In der Veröffentlichung wird mit einem Inception Net gearbeitet. Das Inception Netz wird in Kapitel \ref{sec:inception} vorgestellt.

In der ersten Situation wird der Operator für ein breiteres Netz evaluiert, in dem das Netz breiter gemacht wird. 
In der zweiten Situation wird der Operator für ein tieferes Netz benutzt um zusätzliche Blöcke hinzuzufügen oder bestehenden Blöcken Schichten hinzuzufügen. In der dritten Situation werden beide Operatoren kombiniert. Mit der Kombination wird der Raum der möglichen Operatoranwendungen erkundet, ausgehend von einem Modell.
Die drei Situationen werden in den drei folgenden Unterkapiteln näher beschrieben. Anpassungen der Lernrate werden hier nicht vorgenommen.

\section{Evaluierung des Operators für ein breiteres Netz}
Der Operator für ein breiteres Netz wird evaluiert, in dem ein schmalles Baseline-Netz für 90 Epochen trainiert wird. Dann wird der Operator angewendet. Nach der Anwendung wird für weitere 90 Epochen trainiert. Der Operator für ein breiteres Netz macht das Netz für diese Evaluierung doppelt so breit.


Wie in Kapitel \ref{sec:net2net} beschrieben werden beim Operator für ein breiteres Netz die Gewichte für die neu hinzugefügten Gewichte aus den ursprünglichen Gewichten ausgewählt und so normalisiert, dass die approximierte Funktion nach Anwendung des Operators nicht signifikant von der approximierten Funktion mit den neuen Gewichten abweicht. Um zu evaluieren wie gut diese Methode funktioniert wird sie auf das schmalle Baseline Netz angewendet und verglichen mit dem schmallen und breiten Baseline-Netz. Um die Methode der Initialisierung der zusätzlichen Kanäle zu evaluieren wird als Vergleich ein Netz trainiert, bei welchem die zusätzlichen Gewichte zufällig initialisiert werden.
\begin{figure}
     \centering
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/wider1.png}\label{abb:wider1}}     
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/wider2.png}\label{abb:wider2}}
     \hfill
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/wider3.png}\label{abb:wider3}}
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/baselineS.png}\label{abb:wider4}}
     
     \caption{}
     \label{abb:wider}
\end{figure}

In Abbildung \ref{abb:wider1} ist der Verlauf der Accuracy des schmallen Baseline Netzes in blau und des breiten Baseline Netzes in schwarz über 180 Epochen ohne Anpassung der Lernrate zu sehen. Es fällt auf, dass das schmalle Baseline Netz währenddem Training wesentlich stabiler trainiert als das breite. Allerdings ist das schmalle Baseline-Netz nach wenigen Epochen bereits so weit, dass es kaum Verbesserungen gibt. Auf das schmalle Baseline Netz wird im nächsten Schritt der Operator für ein breiteres Netz angewendet. In Abbildung \ref{abb:wider2} ist der Verlauf der Accuracy des schmallen Baseline Netzes in blau im Vergleich zum schmallen Baseline Netz mit Anwendung des Operators in grün zu sehen. In Abbildung \ref{abb:wider3} ist ein Ausschnitt von Abbildung \ref{abb:wider2} für die Epochen 90 -180 zu sehen. An den beiden Grafiken ist eindeutig abzulesen, dass der Operator für ein breiteres Netz auf dem Baseline Netz die Accuracy verbessert. Die Verbesserung liegt bei den verglichenen Durchläufen bei 2 \%. 

Gegenüber dem breiten Baseline Netz spart dieses Training mit dem Operator dadurch Zeit, dass in den ersten 90 Epochen weniger Trainingszeit verbraucht wird. In Abbildung \ref{abb:wider4} ist die durchschnittliche Trainingszeit des schmallen Baseline Netz verglichen mit dem breiten in einem Boxplot zu sehen. Bei der hier verwendeten Größe des Netzes dauert die Anwendung des Operators für ein breiteres Netz etwa 0,3 Sekunden. Damit liegt die Trainingszeit des Netzes mit Anwendung des Operators zwischen schmallen und breiten Baseline-Netz.


Bei dem hier verwendeten Netz und Datensatz macht die Anwendung des Operators Sinn, dass sie das Training stabiler, schneller und besser macht.





\section{Evaluierung des Operators für ein tieferes Netz}
Zur Evaluierung des Operators für ein tieferes Netz wird zunächst wie in der Veröffentlichung jeder Block um eine Schicht erweitert. Dafür wird mit einem Netz gestartet, welches jeweils nur eine Convolution und Batchnormalisation Schicht pro Block hat $ (l=1)$. 

Dabei werden die zusätzlichen Schichten wie in Kapitel \ref{sec:net2net} beschrieben initialisiert. Als Vergleich dient das Baseline-Netz aus Kapitel \ref{sec:baseline}. Ein weiterer Vergleichspunkt ist das Netz, mit welchem trainiert wird beovr der Operator abgewendet wird. Dieses Netz wird zusätzlich über 180 Epochen trainiert, um die Auswirkungen des Operators zu sehen. Die Accuracy des Ausgangsnetzes ist in Abbildung \ref{abb:deeper11} zu sehen. Zu beobachten ist, dass das Augangsnetz auch unstabil trainiert. 

\begin{figure}
     \centering
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/baselinel1.png}\label{abb:deeper11}}     
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/deeper12.png}\label{abb:deeper12}}
     \hfill
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/deeper13.png}\label{abb:deeper13}}
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/deeper14.png}\label{abb:deeper14}}\\
     \subfloat[][]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/deeper21.png}\label{abb:deeper21}}

     \caption{}
     \label{abb:deeper}
\end{figure}


Wird in Epoche 90 der Operator für ein tieferes Netz angewendet, so wird im Gegensatz zum Operator für ein breiteres Netz danach ein deutliches Abfallen der Accuracy auf unter 30 \% beobachtet. Allerdings lässt sich hier auch eine leicht erhöhte Stabilität des Trainings beobachten. In schwarz ist jeweils das Training des breiten Baseline Netzes zu sehen.  Dies ist Abbildung \ref{abb:deeper12} in grün abgebildet. Die blaue Kurve in Abbildung \ref{abb:deeper13} zeigt eine andere Initialisierungsmethode. Diese fügt zusätzlich noch eine normalverteilte Störung mit $\mu = 0, \sigma^2 = 0.1$ ,den Gewichten die gleich Null sind nach der Initialisierung, hinzu. 

Zu beobachten ist, dass das Netz von vorne anfängt zu trainieren. Trotz dieses Problem wird Zeit gespart, durch die schnellere Trainingszeit bis zum Anwenden des Operators. Da beim Ausgangsnetz bereits recht schnell keine Verbesserung auftritt zeigt Abbildung \ref{abb:deeper14} das Ergebnis bei Anwendung des Operators in Epoche 50 in Grün. 
Es zeigt sich, dass bei der Verwendung dieser Netzstruktur auf dem Datensatz Cifar10 keine wesentliche Verbesserung gegenüber der Netzstruktur nachdem Anwenden des Operators ergibt. Es ergibt sich jedoch eine Verbesserung gegenüber dem Ausgangsnetz und die Stabilität des Trainings steigt.



Eine weitere Verwendung des Operators für ein tieferes Netz ist die Möglichkeit einen neuen Block hinzuzufügen. Nur ein einfaches Hinzufügen würde hier zu Problemen führen, da durch das Hinzufügen eines Blockes die vom Netz berechnete Funktion von der vorher berechneten Funktion abweicht. Gelöst wird dies, in dem alle Blöcke vor der Addition mittels eines muliplikativen Faktors skaliert werden. Wie in Abbildung \ref{abb:diagramm} abgebildet ist, werden alle muliplikativen Faktoren des Ausgangsnetzes auf 1 gesetzt, sodass hier kein Effekt auf die berechnende Funktion entsteht. Beim neu hinzuzufügenden Block werden die beiden multiplikativen Faktoren auf 0,5 gesetzt. Die multiplikativen Faktoren sind trainierbar.

Da der neu hinzuzufügende Block ein Identitätsblock ist wird das Ergebnis des Identitätsblockes und der Kurzschlussverbindung zusammen keinen Effekt auf die vom Netz berechnete Funktion haben. In Abbildung \ref{abb:diagramm} ist zu sehen, wie der neue Block in das Gefüge der anderen Blocks passt.


In Abbildung \ref{abb:deeper21} ist abgebildet, wie sich die Accuracy eines Netzes mit den multiplikativen Faktoren über 180 Epochen verhält. Auch hier ergibt sich zwar eine Steigerung gegenüber dem Ausgangsnetz. Die Accuracy des Ausgangsnetzes ist in Abbilsung \ref{abb:BaselineMul} zu sehen.

Eine Verbesserung gegenüber dem Baseline Netz ergibt sich allerdings nicht. Wie auch schon beim 1. Operator für ein tieferes Netz ergibt sich eine Einsparung in der Trainingszeit.


\begin{figure}
     \centering
     \subfloat[][]{\input{KapitelPartB/Diagramm1}}     
     \subfloat[][]{\input{KapitelPartB/Diagramm2}}
     \subfloat[][Diagramm 3]{\input{KapitelPartB/Diagramm3}}
     \caption{(a) Aufbau eines Blockes mit multiplikativen Faktoren. (b-c) Übersicht wie das Einfügen eines neuen Blockes von statten geht}
     \label{abb:diagramm}
\end{figure}


\begin{figure}[]
     \centering
     \includegraphics[width= .65\textwidth]{KapitelPartB/Images/baselineMul1.png}\label{abb:BaselineMul1}
     \caption{Vergleich der Accuracy vom Baseline Netz und dem Baseline Netz mit multiplikativen Faktoren}
     \label{abb:BaselineMul}
\end{figure}






\section{Erkunden des Modellraums}
Ausgehende von einem Netz mit drei Blöcken pro Phase $(N=[3,3,3])$, Breite der ersten Schicht von 4 $(K=[4,8,16])$ und einer Schicht pro Block $(l=1)$ werden hintereinander im Abstand von 60 Epochen jeweils ein Operator auf das Netz angewendet. In Abbildung \ref{abb:room1} ist ein Boxplot mit der Accuracy nach der Anwendung jeweils eines Operators zu sehen. Entgegen dem vorherigen Kapitel ergibt sich hier auf für den Operator, der einen Block hinzufügt eine ähnliche Verbesserung wie der Operator für ein breiteres Netz. In den Abbildung \ref{abb:roomw1}, \ref{abb:roomd1} und \ref{abb:roomd21} sind die Boxplots für die Accuracy nach dem Anwenden des zweiten Operators zu sehen. 

 \begin{figure}
     \centering
     \subfloat[][]{\input{KapitelPartB/Diagramm4}
     \label{abb:deeperRoom1}}
     \caption{Übersicht über die Anwendung der Operatoren auf das Ausgangsnetz. Beschriftung der Knoten: \{Anzahl an Blöcken pro Phase; Breite der ersten Schicht; Schichten pro Block \}. Tiefer 1 bezeichnet den Operator, der jedem Block eine zusätzliche Schicht hinzufügt. Tiefer 2 bezeichnet den Operator, der jeder Phase einen zusätzlichen Block hinzufügt.}
     \hfill
\end{figure}

Es fällt auf, dass die Reihenfolge, in welcher die Operatoren angewendet werden einen großen Unterschied machen. Außerdem hat jeder Zweig des Baumes, nachdem Anwenden des ersten Operators zumindest ein Blatt, bei dem die Accuracy zugenommen hat.

\begin{figure}
     \centering
     \subfloat[][Anwendung des ersten Operators]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/room1.png}\label{abb:room1}}     
     \subfloat[][Anwendung des zweiten Operators nach Anwenden des Operators für ein breiteres Netz]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/roomw1.png}\label{abb:roomw1}}
     \hfill
     \subfloat[][Anwendung des zweiten Operators nach Anwenden des Operators für ein Netz mit einer zusätzlichen Schicht pro Block]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/roomd1.png}\label{abb:roomd1}}
     \subfloat[][Anwendung des zweiten Operators nach Anwenden des Operators für ein Netz mit einem zusätzlichen Block pro Phase]{\includegraphics[width= .47\textwidth]{KapitelPartB/Images/roomd21.png}\label{abb:roomd21}}

     \caption{}
     \label{abb:room}
\end{figure}
 Obwohl das Ausgangsnetz hier sehr klein ist gibt es durch das Anwenden der Operatoren ein Verbesserungspotential bei den meisten Operatoren.
