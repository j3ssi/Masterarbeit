\chapter{Überblick über die Arbeit}\label{sec:experimente}
Im zweiten Teil dieser Arbeit werden die in Kapitel \ref{sec:wissenschaft} theoretisch betrachteten Methoden praktisch auf einer GPU ausgeführt und evaluiert. Der Überblick über das experimentelle Setup wird in Kapitel \ref{sec:setup} vorgestellt. In Kapitel \ref{sec:konzept} wird das Konzept des praktischen Teils der Arbeit erläutert.

\section{Experimentelles Setup}\label{sec:setup}
In diesem Kapitel wird das Setup für die Experimente vorgestellt. Zunächst wird in Kapitel \ref{sec:hard} die verwendete Hardware vorgestellt. In Kapitel \ref{sec:framework} wird das verwendete Framework erläutert. In Kapitel \ref{sec:archi} wird erläutert, welche Architektur für die Evaluierungen verwendet wird. In Kapitel \ref{sec:baseline} wird für diese Architektur ein Baseline Netz evaluiert.
\subsection{Hardware}\label{sec:hard}
Die Hardware umfasst einen Server mit vier GPUs.
Von diesen vier GPUs haben zwei GPUs jeweils den gleichen Typ:
\begin{itemize}
 \item Geforce GTX 1080 Ti
 \item Geforce RTX 2080 Ti 
\end{itemize}

Beide GPU-Typen arbeiten mit der CUDA Version 10.1. 

Während der Vorbereitung auf diese Experimente hat sich gezeigt, dass Experimente mit einer Geforce GTX 1080 Ti mit den Experimenten der Geforce RTX 2080 Ti nicht vergleichbar sind. Weiterhin lässt sich durch das Verwenden von gemischt präzisen Zahlen nur auf der Geforce RTX 2080 ein Geschwindigkeitsvorteil beim Training feststellen. Aus diesen zwei Gründen wurden alle Experimente auf der Geforce RTX 2080 Ti ausgeführt.  


\subsection{Wahl des Frameworks}\label{sec:framework}

Es wird mit Pytorch gearbeitet, da Pytorch gegenüber anderen Frameworks eine größere Flexibilität erlaubt. Außerdem ist eine fast vollständige Implementierung von PruneTrain in Pytorch geschrieben. Diese wird im nächsten Kapitel untersucht und soweit erweitert, dass es dem Stand im PruneTrain Paper entspricht.

Pytorch bietet mit cudnn und cuda im Hintergrund gute Möglichkeiten die Trainingszeiten einzelner Epochen zu messen und sie so mit einander zu vergleichen.


\subsection{verwendete Netzarchitektur}\label{sec:archi}
Die PruneTrain Implementierung hat initial mehrere verschiedene Netzarchitekturen zur Auswahl:
\begin{itemize}
 \item AlexNet
 \item ResNet 32/50
 \item vgg 8/11/13/16
 \item mobilenet
\end{itemize}

Diese Auswahl an Netzarchitekturen ist zu umfangreich, um alle diese Architekturen auf den vorgestellten Methoden zu evaluieren. Daher wird im Rahmen dieser Arbeit nur auf ResNet gearbeitet. Diese Entscheidung liegt daran, dass ResNets durch ihre Kurzschlussverbindungen gut mit sehr tiefen Netzstrukturen umgehen können, ohne große Klassifikationsleistungsverluste dank Overfitting. Dies ist vor allem wichtig, wenn das Netz mit Hilfe des Operator für tieferes Netz noch tiefer gemacht werden soll. Die ResNet Struktur wird in der Implementierung so verändert, dass angeben werden kann wie tief das Netz sein soll. Das ResNet wird hier nicht mehr nur mit einer Zahl identifiziert sondern es wird angegeben, wie viele
\begin{itemize}
 \item $s$: Anzahl an Phasen, die das ResNet hat
 \item $N=[n_1, \ldots, n_S]:$ Anzahl von Blöcken pro Phase 
 \item $l$: Anzahl von (Conv+Batch)-Layer pro Block
 \item $[k_1, \ldots,k_S ]:$ Breite der Schichten je Phase
\end{itemize}
das jeweilige ResNet hat. Diese Vorgehensweise hat den Vorteil, dass für ein im Verlauf tieferes beziehungsweise breitere Netz eine Vergleichsmöglichkeit besteht. Dies bedeutet, dass das Netz welches im Verlauf entsteht auch direkt erstellt werden kann.


\subsection{Baseline Netz}\label{sec:baseline}

Um die Ergebnisse der Experimente in den folgenden Kapiteln einschätzen zu können wird ein ResNet, ohne Anpassungen um Trainingszeit zu sparen, trainiert. In Tabelle \ref{tab:baseline} ist die Struktur dieses Netze zu sehen. Das breite Baseline-Netz wird dabei für die Evaluierung des Beschneiden des Netzes verwendet. Das schmalere Baseline-Netz wird für die Evaluierung der Methoden, die das Netz breiter machen verwendet.

Das Netz hat drei Phasen $(s=3)$, wobei jeder der Phasen 5 Blöcke hat $(N=[5,5,5])$. Pro Basisblock sind zwei (Conv+Batch)-Schichten vorhanden $(l=2)$. Bei einem Übergangsblock, der als erster Block in einer neuen Phase bei einer Vergrößerung der Bereit beim Phasenübergang genutzt wird ist eine (Conv+Batch)-Schicht mehr vorhanden. Eine grafische Darstellung der Blöcke ist in Abbildung \ref{abb:blocks} zu sehen.




\begin{figure}[]
   \begin{minipage}[b]{.4\linewidth} % [b] => Ausrichtung an \caption
      \includegraphics[width=0.8\linewidth]{KapitelPartB/Images/Basisblock.png}
      \caption{Basisblock}
   \end{minipage}
   \hspace{.1\linewidth}% Abstand zwischen Bilder
   \begin{minipage}[b]{.4\linewidth} % [b] => Ausrichtung an \caption
      \includegraphics[width=0.8\linewidth]{KapitelPartB/Images/Ubergangsblock.png}
      \caption{Übergangsblock}
   \end{minipage}
   \caption{Grafische Darstellung Basis- und Übergangsblock}
   \label{abb:blocks}
\end{figure}

\begin{table}[]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
      &                & \multicolumn{2}{c|}{breites Baseline-Netz} &\multicolumn{2}{c|}{schmales Baseline-Netz} \\ 
Phase & Schicht/Block  & \#Eingangs- & \#Ausgangs-       & \#Eingangs- & \#Ausgangs-    \\
      &                & \multicolumn{2}{c|}{kanäle}     & \multicolumn{2}{c|}{kanäle}  \\ \hline
      & Conv 1 + Bn 1  & 3                & 16           & 3           & 8              \\ \hline \hline
1     & Basisblock     & 16               & 16           & 8           & 8              \\ \hline
      & Basisblock     & 16               & 16           & 8           & 8              \\ \hline
      & Basisblock     & 16               & 16           & 8           & 8              \\ \hline
      & Basisblock     & 16               & 16           & 8           & 8              \\ \hline
      & Basisblock     & 16               & 16           & 8           & 8              \\ \hline \hline
2     & Übergangsblock & 16               & 32           & 8           & 16             \\ \hline
      & Basisblock     & 32               & 32           & 16          & 16             \\ \hline
      & Basisblock     & 32               & 32           & 16          & 16             \\ \hline
      & BasisBlock     & 32               & 32           & 16          & 16             \\ \hline
      & BasisBlock     & 32               & 32           & 16          & 16             \\ \hline \hline
3     & Übergangsblock & 32               & 64           & 16          & 32             \\ \hline
      & Basisblock     & 64               & 64           & 32          & 32             \\ \hline
      & Basisblock     & 64               & 64           & 32          & 32             \\ \hline
      & Basisblock     & 64               & 64           & 32          & 32             \\ \hline
      & Basisblock     & 64               & 64           & 32          & 32             \\ \hline \hline
      & Linear         & 64               & 10           & 32          & 10             \\ \hline
\end{tabular}
\caption{Struktur des Netzes}
\label{tab:baseline}
\end{table}


\subsubsection{Evaluierung des breiteren Baseline-Netzes}
Das Training wird über 180 Epochen durchgeführt. Es werden 5 Experimente durchgeführt. Dabei ergibt sich der in Abbildung \ref{abb:baseAcc1} gezeigte Verlauf der Validierungs-Accuracy für Experiment vier. Bei diesem Training wurde über die gesamten 180 Epochen mit einer Lernrate von 0.1 trainiert. Mit einem Ergebnis von durchschnittlich 81.11 \% über fünf Experimente ist diese Ergebnis leider nicht zufriedenstellend. 

Eine Verkleinerung der Lernrate kann dieses Ergebnis verbessern \cite{CNNBook}. In Abbildung \ref{abb:baseAcc2} wird dargestellt, wie sich der Verlauf ändert durch eine Anpassung der Lernrate bei Epoche 93 und 150. In diesen beiden Epochen wird die Lernrate jeweils auf ein Zehntel verkleinert. 

In Abbildung \ref{abb:baseAcc3} ist ein Boxplot dargestellt, der die Accuracy von jeweils fünf Experimenten mit oder ohne Anpassung der Lernrate vergleicht. Es ergibt sich eine deutliche Verbesserung der Accuracy des Baseline-Netzes von 81.24 \% auf 91.89 \% durch diese Anpassung. Hier wurden die Epochen, zu denen die Lernrate verkleinert wurde manuell ausgesucht. 

\begin{figure}
     \centering
     \subfloat[][]{\includegraphics[width=.49\textwidth]{KapitelPartB/Images/BaseAcc1.png}\label{abb:baseAcc1}}
     \subfloat[][]{\includegraphics[width=.49\textwidth]{KapitelPartB/Images/BaseAcc.png}\label{abb:baseAcc2}}\\
     \subfloat[][]{\includegraphics[width=.49\textwidth]{KapitelPartB/Images/BaseAcc3.png}\label{abb:baseAcc3}}
     \caption{Vergleich zwischen (a) Baseline-Netz ohne Anpassung der Lernrate und (b) Baseline-Netz mit Anpassung der Lernrate in Epoche 93 und 150. (c)  Boxplot der Accuracys}
     \label{abb:BaseAcc}
\end{figure}

Ein weiteres Vergleichskriterium neben der Accuracy sind die durchschnittlichen Trainingszeiten pro Epoche. Für jedes Experiment wird die durchschnittliche Dauer einer Trainingsepoche mittel des arithmetischen Mittels berechnet.
Die Durchschnittswerte über alle 180 Epoche sind für die zehn Baseline Experimente sind in Tabelle \ref{tab:baselineTime} aufgelistet. Die Durchschnittswerte liegen sehr nah beeinander, der Unterschied zwischen dem größten und dem kleinsten Durchschnittswert liegt bei 0,28. Damit ergeben sich zwischen den zehn Experimenten keine signifikante Unterschiede. Es wird daher mit Experiment vier eines der Experimente mit Anpassung der Lernrate ausgesucht um für die folgenden Experimente/ Kapitel als Vergleich zu dienen.
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|} \hline
           & \multicolumn{5}{c|}{Experimente ohne Anpassung}&\multicolumn{5}{c|}{Experimente mit Anpassung} \\
           &\multicolumn{5}{c|}{der Lernrate} &\multicolumn{5}{c|}{der Lernrate}\\
           & 1       & 2      & 3      & 4       & 5       & 1      & 2     & 3      & 4     & 5  \\ \hline 
$\mu$      & 19,49   & 19,53  & 19,50  & 19,48   & 19,84   & 19,57  & 19,56 & 19,53  & 19.53 & 19.62 \\ \hline
\end{tabular}
\caption{Tabelle für Durchschnittswerte und Standardabweichungen der Trainingszeiten der Experimente}
\label{tab:baselineTime}
\end{table}
\subsubsection{Evaluierung des schmalleren Baseline-Netzes}

Das schmalere Baseline-Netz wird verwendet, um für MorphNet und Net2Net eine schmale Variante zu haben. Der Grund hierfür ist, dass bei einer durchschnittlichen Accuracy von 93,19 \% des breiten Baseline-Netzes nicht mehr viel Raum für Verbesserungen bleibt. In Abbildung \ref{abb:baseAcc3} sind die Experimente für das breite und schmale Baseline-Netz mit der Anpassung der Lernrate gegenüber gestellt. Der Unterschied vom breiten zum schmalen Netz ist ein Accuracy-Verlust von 3,1 \%.    


In Abbildung \ref{abb:baseAccS1} ist abgebildet, wie sich die Accuracy für das schmallere Netz verhält, bei der gleichen Anpassung der Lernrate wie beim breiteren Baseline-Netzen. Der Unterschied in der Accuracy zwischen dem schmalen und breiten Baseline-Netz ist in Abbildung \ref{abb:baseAccS2} abgebildet. In Abbildung \ref{abb:baseAccV} wird die Accuracy des schmalen Baseline Netzes ohne Anpassung der Lernrate mit dem schmalen Baseline Netz mit Anpassung der Lernrate verglichen.


\begin{figure}
     \centering
     \subfloat[][]{ \includegraphics[width=0.5\textwidth]{KapitelPartB/Images/baseAccS.png}\label{abb:baseAccS1}}
     \subfloat[][]{ \includegraphics[width=0.5\textwidth]{KapitelPartB/Images/baseAccV.png}\label{abb:baseAccV}}\\
     \subfloat[][]{\includegraphics[width=.49\textwidth]{KapitelPartB/Images/baseAccSHisto.png}\label{abb:baseAccS2}}
     \caption{Accuracy des schmalen Baseline-Netzes  (a) ohne Anpassung der Lernrate. (b) Boxplot zum Vergleich vom schmalem Baseline Netz ohne Anpassung der Lernrate mit dem schmalen Baseline Netz mit Anpassung der Lernrate (c) Boxplot für den Vergleich der Trainingszeiten des schmallen und breiten Baseline-Netzes}
     \label{abb:BaseAccS}
\end{figure}



\section{Konzept}\label{sec:konzept}

In den nachfolgenden Kapiteln wird ein Konzept erarbeitet, wie MorphNet mit einer Kombination aus PruneTrain und Net2Net verglichen werden kann. MorphNet ist eine Technik, bei der die Struktur des Netzes durch Wiederholtes Verbreitern des Netzes und Verkleinern des Netzes mittels eines Regularisierers gelernt wird.
PruneTrain beschneidet das Netz so, dass unwichtige Gewichte auf Null gesetzt werden mit dem Ziel ganze Kanäle auf Null zu setzen um diese zu Entfernen. Mit der Entfernung von Kanälen und falls alle Kanäle einer Schicht auf Null gesetzt wurde auch ganzen Schichten, soll Trainingszeit gespart werden bei möglichst geringem Accuracy Verlust.
Die Evaluierung von MorphNet wird in Kapitel \ref{sec:morphexperimente} durchgeführt. In Kapitel \ref{sec:ptexperimente} wird das PruneTrain Verfahren evaluiert.  Anschließend wird in Kapitel \ref{sec:net2netexperimente} das Net2Net Verfahren evaluiert. In Kapitel \ref{sec:fazit} wird erarbeitet, wie die Kombination aus PruneTrain und Net2Net gestaltet werden könnte zusätzlich wird das Ergebnis der Arbeit zusammengefasst.

Das vergleichen von Laufzeiten geschieht mit statistischen Methoden. Beim Vergleich der Trainingszeiten ist von Interesse, ob die Mittelwerte zweier Experimentengruppen unterschiedlich oder gleich sind. Um dies zu entscheiden wird eine Nullhypothese $H_0$ und eine Alternativhypothese $H_1$ aufgestellt\cite{stat}. 
Die Nullhypohese ist für den Fall der Trainingszeiten die Hypothese, dass die Mittelwerte der durchschnittlichen Trainingszeit beider Gruppen gleich sind\cite{stat}. Die Alternativhypothese ist das Gegenteil der Hullhypothese, die Ungleichheit der Mittelwerte\cite{stat}. Um die Alternativhypothese annehmen zu können muss die Nullhypothese abgelehnt werden\cite{stat}.


Um zu entscheiden, ob die Nullhypothese abgelehnt werden kann ist ein statistischer Test notwendig\cite{stat}. Für die vorliegenden Daten wird ein t-Test verwendet. Beim t-Test wird der T-Score berechnet, der angibt wie groß die Differenz zwischen den beiden Gruppen ist \cite{stat}. Aus dem t-Score wird der p-Wert berechnet. Der p-Wert gibt an wie groß die Wahrscheinlichkeit für einen Fehler 1.Art ist. Der Fehler 1. Art ist die Wahrscheinlichkeit dafür, dass die Nullhypothese zurückgewiesen wird, obwohl diese wahr ist\cite{stat}.
