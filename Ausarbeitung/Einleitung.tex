\chapter{Einleitung}
\label{sec:EinleitungGesamt}


In diesem Kapitel wird zunächst die Motivation und das Ziel dieser Arbeit in Kapitel \ref{sec:first} erläutert. In Kapitel \ref{sec:ergebnis} werden die Ergebnissen dieser Arbeit kurz vorgestellt. Abschließend wird in Kapitel \ref{sec:aufbau} ein Überblick über den Aufbau dieser Arbeit gegeben.

\section{Motivation und Ziel dieser Arbeit}\label{sec:first}
Wird die Zeit-Effizienz des Trainings eines Convolutional Neural Networks betrachtet, so ist die Betrachtung mehrerer Größen möglich. Es kann gemessen werden, wie lange eine Epoche des Netzes trainiert. Außerdem kann gemessen werden, wieviel Trainingszeit beziehungsweise Durchgänge gebraucht werden, bis die Hyperparameter gefunden wurde, die ein zufriedenstellendes Ergebnis für die Testdaten ergeben. Im Rahmen dieser Arbeit soll evaluiert werden, welche Möglichkeiten es gibt, um den Prozess des Findens dieser Hyperparameter zu automatisieren und damit zu beschleunigen. Ohne diese Automatisierung muss nach jeden Trainingsdurchgang, bei nicht zufriedenstellenden Ergebnissen, manuell eine Anpassung der Hyperparmeter erfolgen. 


Ein bestehendes Konzept, welches diesen Ansatz bereits verfolgt ist das "`schnelle Ressourcen beschränkte Struktur lernen tiefer Netzwerke"', kurz MorphNet \cite{morphnet}. Mit MorphNet wird mittels zwei konkurrierender Verfahren die Struktur des Netzes bestimmt. Die konkurrierenden Verfahren sind das Breiter machen des gesamten Netzes mittels eines Breitenmultiplikator und das Beschneiden des Netzes mit Hilfe eines Regulierers. 


Neben diesem Verfahren wird ein eigenes Konzept erarbeitet für ein alternatives Verfahren, mit ebenfalls zwei konkurrierenden Verfahren. Der Vorteil dieses alternativen Verfahrens gegenüber MorphNet ist, das neben der Breite auch die Tiefe des Netzes geändert werden kann. In diesem neuen Verfahren wird als beschneidende Methode PruneTrain benutzt, welches zusätzlich zum Trainingsfehler auch die Gewichte zusammenhängender Kanäle gemeinsam minimiert \cite{prunetrain}. PruneTrain hat den Vorteil, dass auch ganze Blöcke des verwendeten ResNet entfernt werden können. Als vergrößerndes Verfahren wird Net2Net benutzt. Net2Net arbeitet mit drei Operatoren, einen der das Netz breiter macht und zwei, die das Netz tiefer machen \cite{net2net}. Dabei werden die zusätzlichen Gewichte so initialisiert, dass das Netz approximativ die gleiche Funktion berechnet. 


\section{Ergebnisse der Arbeit}\label{sec:ergebnis}
MorphNet zeigt bei der Evaluierung auf einem Resnet mit dem Datensatz Cifar10 Schwächen. Diese Schwächen liegen sowohl bei fehlender Flexibilität, was die Tiefe des Netzes angeht als auch beim Umgang mit einem nicht stabil trainierenden Netz.

Die Evaluierung von PruneTrain zeigt, dass bei einfachem Beschneiden des Netzes keine Trainingszeit gespart werden kann. Um hier beim Trainieren Zeit zu sparen, bei gleicher Speicherauslastung muss mit dem Beschneiden des Netzes auch die Batchgröße erhöht werden. Eine weitere Erkenntnis ist, dass in der Regel ein Gewinn an Trainingszeit mit einem Verlust an Accuracy einhergeht.

Bei Net2Net zeigte sich, dass ein flacheres oder schmaleres Netz die Instabilität beim Training verringern kann. Mit Anwendung eines Operators, der das Netz breiter oder tiefer macht wird diese Stabilität erhalten. Der Operator für ein breiteres Netz schafft es, die Accuracy gegenüber einem genauso breiten ohne Änderung trainierten Netz zu verbessern. In der vorliegenden Evaluierungssituation schafft der Operator für ein tieferes Netz dies nicht.
Die Erkundung des Modellraumes zeigt, dass Net2Net auch bei kleinen Ausgangsnetzen funktioniert. Bei der Anwendung von mehreren Operatoren hintereinander ist die Reihenfolge dieser Operatoren wichtig.

Mit diesen Ergebnissen von PruneTrain und Net2Net hat das Konzept aus der Kombination dieser beiden das Potential besser als MorphNet zu werden.

\section{Aufbau der Arbeit}\label{sec:aufbau}
In Kapitel \ref{sec:wissenschaft} wird zunächst der aktuelle Stand der Wissenschaft erläutert. Zu diesem Zweck werden zunächst in Unterkapitel \ref{sec:conv} die Grundlagen und Funktionsweisen eines CNNs erklärt. Die in dieser Arbeit verwendeten CNN-Architekturen ResNet und Inception werden darauf aufbauend in Unterkapitel \ref{sec:archit} beschrieben. In Kapitel \ref{sec:suche} wird eine Bibliometrie zum Thema der Arbeit vorgestellt. Dies ermöglicht dem Leser nachzuvollziehen wie das Fundament auf dem diese Arbeit beruht gefunden wurde. 


In Kapitel \ref{sec:prunetrain} wird das Beschneiden des Netzes vorgestellt, welches Gewichte so minimiert, dass unwichtige Kanäle auf Null gesetzt und dann entfernt werden können ohne große Accuracy Verluste. In Kapitel \ref{sec:net2net} werden die Operatoren vorgestellt, welche das Netz breiter beziehungsweise tiefer machen sollen.


In Kapitel \ref{sec:auto} wird ein Überblick über das Thema der automatischen Architektursuche gegeben, um das Strukturlernen des Netzes im Kontext seiner Forschungsrichtung zu betrachten. In Kapitel \ref{sec:morphnet} wird die Vergleichsmethode vorgestellt, welche mit Hilfe von Strukturlernen versucht eine bessere Netzstruktur zu finden.


In Kapitel \ref{sec:experimente} wird ein Überblick über den praktischen Teil der Arbeit gegeben. Dazu wird zunächst das Setup des praktischen Teils beschrieben in Kapitel \ref{sec:setup}. In Kapitel \ref{sec:konzept} wird das Konzept für die darauf folgenden Kapitel beleuchtet.


Die Evaluierung von MorphNet ist in Kapitel \ref{sec:morphexperimente} zu finden. 
PruneTrain wird in Kapitel \ref{sec:ptexperimente} evaluiert. Anschließend wird Net2Net in Kapitel \ref{sec:net2netexperimente} evaluiert.

Abschließend werden die Erkenntnisse in Kapitel \ref{sec:fazit}
 zusammengefasst und ein Fazit gezogen.
