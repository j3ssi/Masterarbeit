\BOOKMARK [0][]{pdf:title.0}{Front page}{}% 1
\BOOKMARK [0][]{pdf:toc.0}{Inhaltsverzeichnis}{}% 2
\BOOKMARK [0][]{pdf:Notation.0}{Mathematische Notation}{}% 3
\BOOKMARK [0][]{chapter.1}{Einleitung}{}% 4
\BOOKMARK [1][-]{section.1.1}{Motivation und Hintergrund dieser Arbeit}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.2}{Aufbau der Arbeit}{chapter.1}% 6
\BOOKMARK [0][]{chapter.2}{Stand der Wissenschaft}{}% 7
\BOOKMARK [1][-]{section.2.1}{Funktionsweise eines CNN}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.2}{\334berblick \374ber die g\344ngigen Methoden}{chapter.2}% 9
\BOOKMARK [2][-]{subsection.2.2.1}{Suchbegriffe}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.2}{verwendete Datensets}{section.2.2}% 11
\BOOKMARK [1][-]{section.2.3}{Verringerung der f\374r Berechnungen n\366tige Zeit}{chapter.2}% 12
\BOOKMARK [2][-]{subsection.2.3.1}{Berechnung mit 16 Bit Gleitkomma}{section.2.3}% 13
\BOOKMARK [2][-]{subsection.2.3.2}{Berechnung mit 16 Bit Dynamischen Festkommazahlen}{section.2.3}% 14
\BOOKMARK [1][-]{section.2.4}{Beschleunigung der Berechnung des Gradientenabstiegsverfahren}{chapter.2}% 15
\BOOKMARK [2][-]{subsection.2.4.1}{Accelerating CNN Training by Sparsifying Activation Gradients}{section.2.4}% 16
\BOOKMARK [2][-]{subsection.2.4.2}{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}{section.2.4}% 17
\BOOKMARK [2][-]{subsection.2.4.3}{Accelerating Deep Neural Network Training with Inconsistent Stochastic Gradient Descent}{section.2.4}% 18
\BOOKMARK [2][-]{subsection.2.4.4}{Accelerated CNN Training Through Gradient Approximation}{section.2.4}% 19
\BOOKMARK [1][-]{section.2.5}{Verfahren um weniger Trainingsdaten zu verwenden}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.5.1}{Stochastisches Pooling}{section.2.5}% 21
\BOOKMARK [2][-]{subsection.2.5.2}{Lernen von Struktur und St\344rke von CNNs}{section.2.5}% 22
\BOOKMARK [1][-]{section.2.6}{Strukturelle Ver\344nderungen zur Beschleunigung des Trainings}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.6.1}{Pruning um Trainingszeit zu minimieren}{section.2.6}% 24
\BOOKMARK [2][-]{subsection.2.6.2}{Net 2 Net}{section.2.6}% 25
\BOOKMARK [2][-]{subsection.2.6.3}{Kernel rescaling}{section.2.6}% 26
\BOOKMARK [2][-]{subsection.2.6.4}{Resource Aware Layer Replacement}{section.2.6}% 27
\BOOKMARK [1][-]{section.2.7}{Weitere Herangehensweisen}{chapter.2}% 28
\BOOKMARK [2][-]{subsection.2.7.1}{Tree CNN}{section.2.7}% 29
\BOOKMARK [2][-]{subsection.2.7.2}{Standardization Loss}{section.2.7}% 30
\BOOKMARK [2][-]{subsection.2.7.3}{Wavelet}{section.2.7}% 31
\BOOKMARK [0][]{chapter.3}{Experimentelle Untersuchung der m\366glichen Strategien}{}% 32
\BOOKMARK [1][-]{section.3.1}{Experimentales Setup}{chapter.3}% 33
\BOOKMARK [1][-]{section.3.2}{\334berblick \374ber die m\366glichen Strategien}{chapter.3}% 34
\BOOKMARK [2][-]{subsection.3.2.1}{Zahlenformate}{section.3.2}% 35
\BOOKMARK [2][-]{subsection.3.2.2}{Beschleunigung der Berechnung des Gradientenabstiegverfahren}{section.3.2}% 36
\BOOKMARK [2][-]{subsection.3.2.3}{Verfahren um weniger Trainingsdaten zu verwenden}{section.3.2}% 37
\BOOKMARK [2][-]{subsection.3.2.4}{Strukturelle Ver\344nderungen}{section.3.2}% 38
\BOOKMARK [2][-]{subsection.3.2.5}{andere Herangehensweisen}{section.3.2}% 39
\BOOKMARK [2][-]{subsection.3.2.6}{Tensorflow vs. PyTorch}{section.3.2}% 40
\BOOKMARK [1][-]{section.3.3}{Evaluation der Ergebenisse}{chapter.3}% 41
\BOOKMARK [0][]{chapter.4}{Konklusion}{}% 42
\BOOKMARK [-1][]{part.1}{Additional information}{}% 43
\BOOKMARK [0][]{chapter*.18}{Abbildungsverzeichnis}{part.1}% 44
\BOOKMARK [0][]{chapter*.19}{Algorithmenverzeichnis}{part.1}% 45
\BOOKMARK [0][]{chapter*.20}{Quellcodeverzeichnis}{part.1}% 46
\BOOKMARK [0][]{chapter*.20}{Literaturverzeichnis}{part.1}% 47
