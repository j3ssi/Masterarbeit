\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{MAL{\etalchar{+}}19}

\bibitem[CCGS16]{CNNImg}
J.F. Couchot, R.~Couturier, C.~Guyeux, and M.~Salomon.
\newblock Steganalysis via a convolutional neural network using large
  convolution filters.
\newblock {\em CoRR}, 2016.

\bibitem[CDZM20]{morphImple}
Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Marculescu.
\newblock Towards efficient model compression via learned global ranking, 2020.

\bibitem[CGS15]{net2net}
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.
\newblock Net2net: Accelerating learning via knowledge transfer.
\newblock 11 2015.

\bibitem[DZZZ20]{polsar}
Hongwei Dong, Bin Zou, Lamei Zhang, and Siyu Zhang.
\newblock Automatic design of cnns via differentiable neural architecture
  search for polsar image classification.
\newblock {\em IEEE Transactions on Geoscience and Remote Sensing}, 2020.

\bibitem[FC19]{lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em ICLR}, 2019.

\bibitem[GBC16]{CNNBook}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep Learningo}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[GEN{\etalchar{+}}18]{morphnet}
A.~{Gordon}, E.~{Eban}, O.~{Nachum}, B.~{Chen}, H.~{Wu}, T.~{Yang}, and
  E.~{Choi}.
\newblock Morphnet: Fast simple resource-constrained structure learning of deep
  networks.
\newblock In {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2018.

\bibitem[Gos08]{stat}
William~Sealy Gosset.
\newblock The probable error of a mean.
\newblock {\em Biometrika}, 6, March 1908.

\bibitem[Hay98]{neural}
S.~Haykin.
\newblock {\em Neural Networks: A Comprehensive Foundation}.
\newblock Prentice Hall PTR, Upper Saddle River, NJ, USA, 2nd edition, 1998.

\bibitem[HZRS15]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em CoRR}, 2015.

\bibitem[IS15]{batchnorm}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML’15, page
  448–456. JMLR.org, 2015.

\bibitem[LCZ{\etalchar{+}}19]{prunetrain}
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Mattan Erez, and Sujay
  Shanghavi.
\newblock Prunetrain: Gradual structured pruning from scratch for faster neural
  network training.
\newblock {\em CoRR}, abs/1901.09290, 2019.

\bibitem[MAL{\etalchar{+}}19]{dvolver}
Guillaume Michel, Mohammed~Amine Alaoui, Alice Lebois, Amal Feriani, and Mehdi
  Felhi.
\newblock {DVOLVER:} efficient pareto-optimal neural network architecture
  search.
\newblock {\em CoRR}, abs/1902.01654, 2019.

\bibitem[oA19]{cornell}
ohne Autor.
\newblock arxiv machine learning classification guide, 12 2019.
\newblock Onlinequelle; Aufgerufen am 01.06.2020;
  \url{https://blogs.cornell.edu/arxiv/2019/12/05/arxiv-machine-learning-classification-guide/}.

\bibitem[ptI]{ptImpl}
Prune train implementierung.
\newblock online
  \url{https://bitbucket.org/lph_tools/prunetrain/src/master/}aufgerufen am
  10.06.2020.

\bibitem[SG17]{popular}
Charles~A. Sutton and Linan Gong.
\newblock Popularity of arxiv.org within computer science.
\newblock {\em CoRR}, 2017.

\bibitem[SLJ{\etalchar{+}}15]{inception}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock {\em 2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[SXZ{\etalchar{+}}20]{gen}
Yanan Sun, Bing Xue, Mengjie Zhang, Gary~G Yen, and Jiancheng Lv.
\newblock Automatically designing cnn architectures using the genetic algorithm
  for image classification.
\newblock {\em IEEE Transactions on Cybernetics}, 2020.

\bibitem[TKYG20]{snyc}
Hidenori Tanaka, Daniel Kunin, Daniel L.~K. Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow, 2020.

\end{thebibliography}
