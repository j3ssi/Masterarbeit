%========================================================================================
% Latex-Beamer-Template
% TU Dortmund, Informatik Lehrstuhl VII
%========================================================================================
\documentclass[10pt]{beamer}

\usetheme{tufi} 
\usepackage{amsmath}

\usepackage{wasysym}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{verbatim}
\usepackage[babel,german=quotes]{csquotes}
\usepackage{array}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{pgfpages}
\usepackage[backend=biber]{biblatex}
\bibliography{Literatur.bib}

\newcommand\tabrotate[1]{\begin{turn}{90}\rlap{#1}\end{turn}}
\newcommand\MyBox[2]{
  \fbox{\lower0.75cm
    \vbox to 1.7cm{\vfil
      \hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
      \vfil}%
  }%
}

%========================================================================================
% Hier Vortragstitel, Autor und Vortragsdatum eintragen
\pdfinfo
{
  /Title       (Zeit-Effizientes Training von Convolutional Neural Networks)
  /Creator     (TeX)
  /Author      (Jessica Bühler)
}


\title{Masterarbeit -- Zeit-Effizientes Training von Convolutional Neural Networks}
\subtitle{Zwischenstand}
\author{Jessica Bühler}
\date{\today}
%========================================================================================
\begin{document}

\frame{\titlepage}

\AtBeginSection[]
{
  \frame<handout:0>[allowframebreaks]
  {
    \frametitle{Übersicht}
    \tableofcontents[currentsection,hideallsubsections]
  }
}

\AtBeginSubsection[]
{
  \frame<handout:0>[allowframebreaks]
  {
    \frametitle{Übersicht}
    \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
  }
}

\newcommand<>{\highlighton}[1]{%
  \alt#2{\structure{#1}}{{#1}}
}

\newcommand{\icon}[1]{\pgfimage[height=1em]{#1}}


%=Inhalt=================================================================================
\begin{frame}{Zusammenhang Hardware und Zeitersparnis}
 In der Praxis wird für das Trainieren eines CNNs in der Regel eine Grafikkarte genutzt.
 Für die Bearbeitung meiner Masterarbeit stehen mir momentan zur Verfügung:
 \begin{itemize}
  \item Geforce 750Ti mit 2 Gb mit CUDA-Version 5.0
  \item Geforce 1070 mit 11 Gb mit CUDA-Version 6.1
  \item Lehrstuhlserver mit ?? mit ?? Gb mit CUDA-Version ??
 \end{itemize}
 
 Die großen Unterschiede im Speicher machen es notwendig unterschiedlich große Netze zu trainieren. Wie gross ist der Unterschied im Gewinn bei der Trainingszeit?
\end{frame}

\section{Prune Train}

\begin{frame}{Code}
 https://bitbucket.org/lph\_tools/prunetrain/src/master/
\end{frame}



\begin{frame}{Was ist Pruning?}
\begin{itemize}
 \item Modell Pruning reduziert die Zahl der zu lernenden Parameter in einem dichten Netzwerk, sodass die Inferenzkosten und die Speicherlast reduziert werden.Dabei soll so wenig wie möglich Accuracy verloren gehen.
 \item Hauptziel ist als das Verbessern der Inferenz, aber auch das Training kann signifikant schneller werden.
\end{itemize}
\end{frame}

\begin{frame}{Pruning währenddem Training -- bisherige Vorgehensweise}
\begin{itemize}
 \item Man kann durch das Hinzufügen von Normalisationstermen prunen. Hierfür kann eine $L_1$-Norm oder ein sog. ``group lasso'' verwendet werden.
 \item gropu lasso verwendet $L_1$ oder $L_2$ Normen von Gruppen von Gewichten für strukturelles Pruning. Daher werden vom Optimierungsprozess hier kleine absolute ?? Werte für Gewichte oder Gruppen von Gewichten bevorzugt. Dies für zu einem nicht mehr dichten Netz.
 \item Durch das beibehalten der nicht mehr dichten Sturuktur kommtes allerdings nicht zu Gewinnen im Trainingsprozess.
 \item wird die dichte Struktur nicht beibehalten so wurde bisher komplexes Dataindexing notwendig, welches die Performance verkleinert. 
\end{itemize}
\end{frame}

\begin{frame}{Pruning währenddem Training -- neue Vorgehensweise}
\begin{itemize}
 \item Eine Vorarbeit rekonfiguiert das CNN genau einmal währenddem Training und arbeitet danach mit dem kleineren Modell. Allerdings ist der Rekonfigurierungszeitpunkt vorher nicht festgelegt, was diese Herangehensweise problematisch oder sogar kontraproduktiv macht.
 \item Daher nun Prune Train mit:
 \begin{itemize}
  \item Beschleunigung des Trainingsmechanismus des CNN welches das Model währenddem Training pruned.
  \item Der Prune-Prozess startet bereits bei der ersten Epoche
  \item Es wird ein sog. ``group lasso'' Regulierungsverfahren als Basis für die Linienausdünnung benutzt. Die Gewichte werden regelmässig gepruned.
  \item Das CNN wird regelmässig reconfiguriert um das Modell kleiner und so das Training schneller zu machen.
 \end{itemize}
\end{itemize}
 
\end{frame}


\begin{frame}{Die 3 zentralen Optimierungsverfahren von PruneTrain}
  \begin{itemize}
   \item eine systematische Methode zur Berechnung des group lasso Regularisierung Sanktions Koeffizienten beim Beginn des Trainings.
   \item Kanal union, ein Speicheraufruf kosteneffizientes und Index-freies Kanal Pruning Verfahren für moderne CNNs mit Kurzschlussverbindungen.
   \item Ein dynamische Mini-Batch Adjustment, dass die Größe des Mini-Batch anpasst. Dies geschieht durch beobachten des Speicherkapazitätgebrauchs einer Trainingsiteration nach jeder Pruning reconfiguration.
  \end{itemize} 
  \end{frame}


\begin{frame}{group lasso Regularisierung Sanktions Koeffizienten}
\begin{itemize}
 \item Der group lasso Regularisierung Sanktions Koeffizienten ist ein Hyperparameter, der einen Trade-off  zwischen der Modellgröße und der Accuracy bildet.
 \item Voherige Arbeiten suchen nach einem geeignetem Sanktionsmaß, was das Einbeziehen des Prunings vom Anfang des Trainings sehr teuer macht.
 \item Unser Mechanismus kontrolliert die Group lasso Regularisierungstärke und erreicht eine hohe Modellpruningrate mit nur einem kleinen Einfluss auf die Accuracy bei nur einem Trainingsdurchlauf.
\end{itemize}
\end{frame}


\begin{frame}{Kurzschlussverbindungen und Union Channel}
\begin{itemize}
 \item Kurzschlussverbindungen werden in modernen CNNs häufig genutzt.
 \item Prunning aller genullten Kanäle solcher CNNs brauchen regelmässige Tensor Umordnung um die Kanalindizes zwischen den Schichten zu matchen. Dies vermindert die Performance.
 \item Diese Umordnung wird durch den Channel Union Algorithmus vermieden. Daher folgt eine 1.9 fache Beschleunigung des Convolutional Layetrs.
\end{itemize}
\end{frame}

\begin{frame}{Dynmaisches Minibatch Adjustment}
\begin{itemize}
 \item Dynmaisches Mini Batch Adjustment kompensiert die verminderte Datenparallelität aufgrund des kleineren geprunten Modells durch Erhöhung der Mini-Batch Größe.
 \item Dies sorgt sowohl für bessere Ausnutzung der Hardware ressourcen als auch zur Reduzierung des KOmmunikation overheads durch eine Verminderte Modell Update Frequenz.
 \item Beim Erhöhen der Mini-Batch Größe wird auch die Lernrate mit demselben Verhältnis erhöht, um die Accuracy nicht zu verändern.
 \end{itemize}
 
\end{frame}


\begin{frame}{Pruning Algorithmen}
Pruning Algorithmen sind
\begin{itemize}
 \item strukturiert oder
 \item unstrukturiert.
\end{itemize}
Unstrukturiertes Pruning kann die maximale Modellverkleinerung erarbeiten braucht aber feine Indexing. Daher ineffizient.

Struktuiertes Pruning entfernt oder reduziert feines Indexing und sorgt für ein besseres Ausnutzen der Hardware und realisiert daher einen Performancegewinn.
\end{frame}


\begin{frame}[allowframebreaks]{Strukturiertes Pruning}
In diesem Paper werden zwei Arten von strukturiertem Pruning gezeigt:
\begin{itemize}
 \item Versuch und Irrtum basiertes strukturelles Pruning: Startes mit einem vortrainiertem dichten Modell und versuche dann Gewichte in einem strukturellen Vorgehen zu entfernen, sodass eher ganz Kanäle entfernt werden als einzelne Gewichte. Nicht wichtige Kanäle werden basierend auf ihrem Wert der Gewichte oder wegen Hinweise durch die Regression entfernt.Die entfernten Knäle können wieder eingebunden werden, falls der Verlust an accuracy zugroß ist. Diese Verfahren ist zwar effective, allerdings vergrössert sich der Suchraum mit der Komplexität des Modells. Dadurch kann die Pruning Zeit signifikant anwachsen. Ausserdem sorgt diese Verhfaren durch die Vewendung eines vortrainierten Netzes zu keiner Zeiteinsparung beim Training.
    \item Ein alternativer Mechanismus nutzt Parameterregularisierung. Dies optimiert den Trainingsfehler und sorgt simultan dafür, dass die absluten Werte der Gewichte oder von Gruppen von Werten gegen Null gehen.  Group lasso Regularisierung wird benutzt um die Gewichte strukturell zu verdünnen durch die Zuweisung einer Regularisierungssanktion zu $L_2$ Normen von Gruppen von Gewichten Der Regularisierungsbasierte Pruning Ansatz fügt noch ein Regularisierungsfunktion zum Fehlermass hinzu und minimiert bestimmte Gewichte oder Gruppen davon durch die Backpropagation. Eventuell können Gewichte sogar ganz auf Null gesetzt und dann gepruned werden.

 \end{itemize}
\end{frame}

\begin{frame}{Mathematische Grundlagen -- Baseline Pruning}
 $$ \underset{min}{W} \left( \frac{1}{N} \sum_{i=1}^{N} l(y_i,f(x_i, W)) + \sum_{g=1}^{G} \lambda_g \cdot || W_g ||_2 \right) $$
 \begin{itemize}
  \item $ \frac{1}{N} \sum_{i=1}^{N} l(y_i,f(x_i, W))$ Standard-Kreuzentropie
  \item $\sum_{g=1}^{G} \lambda_g \cdot || W_g ||_2 $ group lasso Regulierungsterm
  \item $f(x_i)$ Vorhersage des Netzwerks auf Eingabe $x_i$
  \item $W$ Gewichte
  \item $l$ Verlustfunktion der Klassifikation und Grundwahrheit $y_i$
  \item $N$ Minibatchgröße
  \item $G$ Zahl von Gruppen
  \item $\lambda$ Verdünnungskoeffizient
 \end{itemize}

\end{frame}




\begin{frame}{Mathematische Grundlagen -- Group lasso Design}
$$ \lambda \cdot \sum_{l=1}^{L} \left( \sum_{c_l=1}^{C_l} || W_{c_l,:,:,:} ||_2 + \sum_{k_l=1}^{K_l} || W_{:,k_l,:,:}||_2 \right) $$ 
Design eines speziellen Groupo Lasso Regulierers, der Gewichte jedes Kanals (Input oder Output) und jeder Schicht gruppiert. $\lambda$ wird als einziger globaler Regularisierungsfaktor gewählt, da so der Fokus auf dem Vermindern der Rechenzeit liegt und nicht auf der Modellgröße. Dies hat zur Folge, dass vorallem große Features verdünnt werden, was zu einer größeren Verminderung der Rechenleistung führt.
\end{frame}


\begin{frame}{Setup des Regularisierungssanktionskoeffizienten}
 Um die Lasso Group Regularisierung vom Anfang des Trainings zu benutzen sollteder Koeffizient $\lambda$ sinnvoll gewählt werden. Dies sorgt für eine hohe Vorhersageaccuracy und einer hohen Pruning Rate. Um zeitintensives Hyperparametertuning zu vermeiden wird hier eine neue Methode eingeführt:
 
 $$LPR=\frac{\lambda \sum_{g}^{G}|| W_{g,:} ||}{l(y_i,f(x_i,W))+\lambda \sum_{g}^{G}||W_{g,:} ||)} $$
 
 Berechnet wird dies durch setzen von zufälligen Werten, mit denen die Gewichte initialisiert werden. LPR wird einmal berechnet und dann bis zum Ende weiter benutzt.
\end{frame}


\begin{frame}{Schichtentfernung durch Überlappende Regulierungsgruppen}
 Ist nicht notwendig, da Schichten die nicht relevant sind eh irgendwann gepruned werden.
\end{frame}

\begin{frame}{Frühes Gewichtspruning}
 Gewicht auf 0 -> Gradient auf 0 -> Erholung sehr unwahrscheinlich -> Daher kann ein auf 0 gesetztes Gewicht gepruned werden.
\end{frame}



\begin{frame}{Robustness des Reconfigurationsintervalls}
 Nach jedem solchen Intervall werden Input und Outputkanäle die 0 sind gepruned. Um ein Missverhältnis zwischen den Dimensionen zu verhindern wird nur die Verbindung von 2 verdünnten Kanälen von 2 aufeinanderfolgenden Schichten gepruned. Alle Trainingsvariablen bleiben gleich.
 Das Reconfigurationsintervall ist ist der einzige zusätzliche Hyperparameter. Zu gross gewählt würde der Intervallparameter zu wenig Zeitverbesserung bringen. Zu klein gewählt könnte er die Lernqualität beeinflussen.
\end{frame}

\begin{frame}{Channel Union vs. Channel Gating}
\begin{itemize}
 \item Channel Union mildern das Vanishing Gradient Problem und öerlauben so tiefe Netze und hohe Accuracy.
 \item Hier werden 2 Ansätze benutzt, um die passende Dimesionierung zu gewährleisten:
 \begin{itemize}
  \item Channel Gating Schichten
  \item Channel Union
 \end{itemize}
 \item Channel Gating braucht die schon erwähnt Umordnung der Tensoren, Channel Union nicht.
 \item Channel Union prunt nur die Verbindungen von verdünnten Kanälen von allen benarchbarten Schichten in einer Rest-Phase.
 \item Channel Union wird benutzt da es mehr Rechenzeit spart als Channel Gating
\end{itemize}
\end{frame}


\begin{frame}{Dynamisches Mini-Batch Adjustment}
Bei der Evaluierung zeigt sich, dass die erhofften Effekte des Mini Batch Adjustment passieren.
\end{frame}


\begin{frame}{Evaluierung}
\begin{itemize}
 \item 4 Matriken zur Evaluierung: Training und Inference FLOPs, gemessenen Trainingszeit, und Validierungsaccuracy. 
\end{itemize}

Training time does not include network architecture reconfiguration time, which we do optimize and occurs only once in many
epochs. We compare the training results of ResNet and VGG using
PruneTrain with the dense baseline. We use the same number of
training iterations for both the dense baseline and PruneTrain to
show the actual training time saved by PruneTrain. We use 182
epochs [15] and 90 epochs to train CNNs on CIFAR and ImageNet,
respectively.
For ResNet32 and ResNet50 on CIFAR10, PruneTrain reduces the
training FLOPs by ∼50% with a minor accuracy drop compared to
the dense baseline. The compressed models after training show only
34% and 30% of the dense baseline inference cost for ResNet32 and
ResNet50, respectively. The results of ResNet32/50 on CIFAR100
show similar patterns, which exhibits the robustness of PruneTrain,
given that CIFAR100 is a more difficult classification problem. For
CIFAR100, PruneTrain reduces the training and inference FLOPs
by 32% and 46% for ResNet32, and 53% and 69% for ResNet50, while
losing only 1.4% and 0.7% of validation accuracy, respectively com-
pared to the dense baseline. These results show that PruneTrain
reduces more training FLOPs from a deeper CNN model, since
more unimportant channels and layers are sparsified and removed
early in the training. PruneTrain also achieves high model compres-
sion with similar validation accuracy loss for both VGG models on
CIFAR.
PruneTrain also shows high training cost savings for ResNet50
trained on ImageNet: 40%, 30%, and 3% for three different prun-
ing strengths (0.25, 0.2, and 0.1). Thus, we conclude that Prune-
Train is robust to changes in CNN model and dataset complexity.
The trained ResNet50 shows 53%, 44%, and 12% reduced inference
\end{frame}


\begin{frame}{Conclusion}
 
\end{frame}




\end{document}
